\documentclass[12pt]{article}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{cite}
\pagestyle{myheadings}
\author{Joseph McDonald}

\textwidth=7in
\textheight= 9in

\topmargin=-0.5in
\oddsidemargin=-0.25in

\newcommand{\xbar}{\bar{x}}
\newcommand{\V}[1]{\boldsymbol{#1}}
\newcommand{\E}[0]{\mathbb{E}}
\title{Parallelizing AdaBoost and Decision Trees}

\begin{document}
\maketitle

\section{Introduction}
Binary and multiclass classification is a fundamental problem in machine
learning and is commonly one of the first concepts to be studied in an
introductory course, the goal being to accurately predict the label of a given
observations using the data associated with the observation. In the context of
binary classification, a labeled observation (or sample point) is a coordinate
pair $(x, y)$ where $x\in\mathbb{R}^d$ is a vector of data associated with the
observation (feature vector) and $y\in\{1, -1\}$ is the corresponding label of
the observation. This classification, and even prediction in general, commonly
splits up into two different scenarios, supervised and unsupervised learning.
Supervised learning involves taking a set of labeled observations (training
set) and using it to construct a prediction function, one that can take a new
feature vector and predict the corresponding label. There are many methods used
for classification in supervised learning, including support vector machines,
decision trees, online algorithms, and many others.

A weak learning algorithm, or weak learner, is an algorithm that, given a set of training data, yields a predictor with performance of at least 50\% percent accuracy, or just better than random guessing. The prediction functions that a weak learner returns are called base classifiers. There was an important question asking whether the base classifiers from a weak learner could somehow be joined together to produce a strong learner, or an algorithm that gives a prediction function of arbitrary accuracy.

Boosting is a category of methods that addresses this question, which Freund and Schapire
successfully answered with their algorithm AdaBoost. AdaBoost, which stands for adaptive boosting, builds
a more accurate predictor through successive rounds of training a weak learner
to the data and subsequently re-weighting the training samples \cite{no1}. 




\begin{thebibliography}
\ref[no1] Rob Schapire

\end{thebibliography}
\end{document}



