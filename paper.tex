\documentclass[12pt]{article}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{cite}
\pagestyle{myheadings}
\author{Joseph McDonald}

\textwidth=7in
\textheight= 9in

\topmargin=-0.5in
\oddsidemargin=-0.25in

\newcommand{\xbar}{\bar{x}}
\newcommand{\V}[1]{\boldsymbol{#1}}
\newcommand{\E}[0]{\mathbb{E}}
\title{Parallelizing AdaBoost and Decision Trees}

\begin{document}

%\begin{abstract}
%I present a parallelized construction of decision trees to be used in AdaBoost
%\end{abstract}

\maketitle

\section{Introduction} Binary and multiclass classification is a fundamental
problem in machine learning and is commonly one of the first concepts to be
studied in an introductory course. In the context of binary classification, a
labeled observation is a coordinate pair $(x, y)$ where $x\in\mathbb{R}^d$ is a
vector of data associated with the observation (feature vector) and $y\in\{1,
-1\}$ is the corresponding label of the observation. For example, $x_i$ might
be a point in $\mathbb{R}^2$ representing the height and weight of the $i$th
individual in a data set while the value of $y_i$ may indicate the individual's
gender. The goal in classification is to accurately predict the label
of a given observation using its feature vector. In this setting, the scenario
of supervised learning involves using a training set of labeled observations
$(x_i, y_i)$ to construct a prediction function $h$, one that takes a feature
vector $\hat{x}$ and gives a prediction on the corresponding label
$h(\hat{x})=\hat{y}$. To check the accuracy of the predictor it is common to use a separate set of test data, i.e. data that hasn't been used for training. There are many methods used to obtain a predictor for classification in supervised
learning, and the class of ensemble methods concerns combining several
predictors together.

Suppose now there exists an algorithm that, for any set of training data, is
guaranteed to give you a predictor that has at least 50\% accuracy. Such an
algorithm is called a weak learning algorithm since it yields a predictor that
performs marginally better than guessing randomly. The prediction functions
returned by a weak learning algorithm are called base classifiers, and a
natural question arises asking whether a combination of weak learners can be used
to make a strong learner, or one that can achieve a certain level of accuracy.
Ensemble methods like this are called boosting, and the
algorithm AdaBoost introduced in \cite{no1} accomplishes this through
successive rounds of training a weak learner to the data and subsequently
re-weighting the training samples \cite{no1}.

The goal of this project is to implement a parallelized version of a commonly
used weak learner, decision trees, to improve the running time of AdaBoost. We
give a short overview of AdaBoost and decision trees in the next section,
describe the implementation of the decision trees, the data sets used and the
results comparing the serial and parallelized execution of the boosting algorithm. 

\section{AdaBoost} The beauty of AdaBoost is in using a relatively inaccurate
weak learning algorithm to produce a much more accurate prediction function. In
each round of AdaBoost for $t$ from 1 to $T$, the weak learner selects a
base classifier $h_t: \mathbb{R}^n\rightarrow\{1,-1\}$ from a set of functions
$H$, in particular choosing the one with the least error
$\epsilon_t=\sum_{i=1}^{n}D_t(i)I(h_t(x_i)\neq y_i)$ (Line 4) where $D_t(i)$ is
the weight associated with the $i$th sample. The algorithm then re-weights the
training samples in Line 8 to place more emphasis on those that that the chosen
classifier $h_t$ incorrectly predicted, so that in future rounds the base
classifiers are tailored to correcting these mistakes. The predictor that is
returned after all rounds is a linear combination of all $h_t$'s. The
coefficient $\alpha_t$ gives more weight in the final predictor to those rounds
where the classifier performed better. $Z_t$ is a normalization constant. We
give the algorithm below.\\ \\
{\sc AdaBoost}: $S = \{(x_i,y_i):i = 1\ldots n\}$
\begin{enumerate}
\itemsep1pt \parskip0pt \parsep0pt
\item {\bf for $i=1$ to $n$}
\item \quad $D_1(i) = \frac{1}{n}$
\item {\bf for $t=1$ to $T$}
\item \quad $h_t$ = {\sc WeakLearner}$(S\sim D_t)$ with weighted error $\epsilon_t$
\item \quad $\alpha_t = \frac{1}{2}\log(\frac{1-\epsilon_t}{\epsilon_t})$
\item \quad $Z_t = 2[\epsilon_t(1-\epsilon_t)]^{1/2}$ (normalization factor)
\item \quad {\bf for $i = 1$ to $n$}
\item \quad \quad $D_{t+1}(i) = \frac{D_t(i)\exp(-\alpha_t y_i h_t(x_i))}{Z_t}$
\item $g = \sum_{t=1}{T} \alpha_t h_t$
\item {\bf return} $\mbox{sign}(g)$
\end{enumerate}

Note that everything except Line 4 can be done relatively quickly, so improving
the speed of the weak learning algorithm will improve the speed of AdaBoost by
a similar factor. 

\section{Decision Trees}
Decision trees are a well-known method used in classification which partitions
the feature space into rectangular regions and gives each region the most
frequent label among the training set points it contains. In the most common
decision tree algorithms each partition of the feature space is made along
coordinate axes and done in a heirarchical way.  Starting with the entire
space, a line parallel to an axis divides it into two regions and then the same
is done for these resulting regions. This process is repeated many times until
the space is partitioned into several rectangles, like the example in Figure 1
(\cite{HTF}). Each split is made to separate points of the two distinct classes
into different regions as best as possible. Thus the two resulting rectangles
will have a greater {\it purity} in comparison to the parent node.

\begin{figure}[b!]
\centering
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=2.5in]{partition}
\caption{space partitioned by a decision tree}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=2.5in]{dectree}
\caption{decision tree}
\end{minipage}
\end{figure}

There are a few common ways to measure purity for a given region containing
sample points. Gini Index is used here which can be easily substituted for
others. For two classes the Gini Index of region $A$ is $I(A) = 2p(1-p)$ where
$p$ is the probability of choosing a point in the first class from $A$, which
translates to the probability of incorrectly guessing the class of a randomly
chosen point from either class.

The splitting of regions gives rise to the tree structure. At each node in the
tree, the samples in the region corresponding to that node are ordered and
counted to find in which dimension that region can be divided that gives the
greatest increase in the purity of the branch. Figure 2 shows an example of
this where at the top of the tree the decision to split on the first feature
$X_1$ is best and the separation is made at $t_1$. In the first of the two
resulting regions it is determined that the best split is made on $X_2$ at
$t_2$, and in the other region the best separation is on $X_1$ at $t_3$. This
splitting process will go on until a stopping criterion is met, either if there
are too few sample points in a region or if no gain in purity can be made by
splitting on any feature.

%In Figure 1, the lines drawn at $t_i$ represent the division of the previous
%region. Since each region is divided in two until a stopping criterion is met,
%this gives rise to a binary tree where the action at each node is a line that
%partitioning the region at that node. At each level of the tree the tree
%corresponding to Figure 1 is given in Figure 2 (\cite{HTF}). As depicted in the
%two figures, the splitting of each node is done in a heirarchical way so that .
%The feature space is partitioned along coordinate axes, which gives rise to
%decision. The most common way to partition the feature space is along
%coordinate axes. 

Since the tree-building process is the most time-consuming portion of AdaBoost,
our goal is to parallelize this. It seems natural to divide amongst processors
the work of finding the dimension in the feature space that best splits the
region and increases the purity for each node in the tree. The datasets we are
handling have several dimensions, on the order of tens to hundreds, so each
processor could be responsible for a group of dimensions in the feature space.
After each processor determines the best dimension to split on amongst it's
group, we can perform a reduction and determine the best split over all
dimensions.



%\maketitle
%
%\section{Introduction}
%Binary and multiclass classification is a fundamental problem in machine
%learning and is commonly one of the first concepts to be studied in an
%introductory course, the goal being to accurately predict the label of a given
%observations using the data associated with the observation. In the context of
%binary classification, a labeled observation (or sample point) is a coordinate
%pair $(x, y)$ where $x\in\mathbb{R}^d$ is a vector of data associated with the
%observation (feature vector) and $y\in\{1, -1\}$ is the corresponding label of
%the observation. This classification, and even prediction in general, commonly
%splits up into two different scenarios, supervised and unsupervised learning.
%Supervised learning involves taking a set of labeled observations (training
%set) and using it to construct a prediction function, one that can take a new
%feature vector and predict the corresponding label. There are many methods used
%for classification in supervised learning, including support vector machines,
%decision trees, online algorithms, and many others.
%
%A weak learning algorithm, or weak learner, is an algorithm that, given a set of training data, yields a predictor with performance of at least 50\% percent accuracy, or just better than random guessing. The prediction functions that a weak learner returns are called base classifiers. There was an important question asking whether the base classifiers from a weak learner could somehow be joined together to produce a strong learner, or an algorithm that gives a prediction function of arbitrary accuracy.
%
%Boosting is a category of methods that addresses this question, which Freund and Schapire
%successfully answered with their algorithm AdaBoost. AdaBoost, which stands for adaptive boosting, builds
%a more accurate predictor through successive rounds of training a weak learner
%to the data and subsequently re-weighting the training samples \cite{no1}. 




%\begin{thebibliography}
%\bibitem{FS} Yoav Freund Rob Schapire

%\bibitem{HTF} Hastie Friedman Tibshirani

%\bibitem{Mohri} Mohri

%\bibitem{no4} Wiki
%\end{thebibliography}
\end{document}



